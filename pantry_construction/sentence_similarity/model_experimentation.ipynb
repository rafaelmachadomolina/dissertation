{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6272f61c",
   "metadata": {},
   "source": [
    "# Experimentation to calibrate model\n",
    "\n",
    "This notebook contains notes and results of several runs in order to determine the best approach to match ingredients in the taxonomy. The idea is to maximise the similarity score on a given sample, starting from a baseline -that is- the most simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d214391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import packages\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62be3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define constants\n",
    "\n",
    "RANDOM_SEED = 888\n",
    "SAMPLE_SIZE = 5000\n",
    "\n",
    "CSV_PATH = 'calibration/csv/'\n",
    "EXCEL_PATH = 'calibration/excel/'\n",
    "JSON_PATH = 'json/'\n",
    "\n",
    "# Filenames\n",
    "FILENAME_RAW_TAXONOMY = 'raw_taxonomy.csv'\n",
    "FILENAME_RAW_INGREDIENTS = 'raw_ingredients.csv'\n",
    "FILENAME_RAW_INGREDIENTS_SAMPLE = 'raw_ingredients_sample.csv'\n",
    "FILENAME_STOPWORDS_EXCEL = 'raw_stopwords.xlsx'\n",
    "FILENAME_STOPWORDS_JSON = 'stopwords.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4334e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define functions\n",
    "\n",
    "def file_exists(filename, directory = ''):\n",
    "    \n",
    "    pwd = os.getcwd()\n",
    "    path = os.path.join(pwd, directory, filename)\n",
    "    \n",
    "    return os.path.isfile(path);\n",
    "                          \n",
    "def load_df_from_csv(filename, directory = ''):\n",
    "    \n",
    "    pwd = os.getcwd()\n",
    "    path = os.path.join(pwd, directory, filename)\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        print(f'Table successfuly loaded from {filename}')\n",
    "        \n",
    "    return df;\n",
    "\n",
    "def generate_string_connection():\n",
    "    \n",
    "    # Retrieve params from env\n",
    "    DB_HOST = os.environ['DB_HOST_STAGE']\n",
    "    DB_NAME = os.environ['DB_NAME_STAGE']\n",
    "    DB_PORT = os.environ['DB_PORT_STAGE']\n",
    "    DB_USERNAME = os.environ['DB_USERNAME_STAGE']\n",
    "    DB_PASSWORD = os.environ['DB_PASSWORD_STAGE']\n",
    "    \n",
    "    # Create string\n",
    "    string_conn = f'mysql+pymysql://{DB_USERNAME}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "    \n",
    "    return string_conn;\n",
    "\n",
    "def load_df_from_sql(tableName, string_conn):\n",
    "    \n",
    "    sqlEngine = create_engine(string_conn)\n",
    "    dbConnection = sqlEngine.connect()\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_sql_table(tableName, dbConnection)\n",
    "        print(f'Query successfully executed for table {tableName}')\n",
    "        return df;\n",
    "\n",
    "    except Exception as ex:   \n",
    "        print(ex)\n",
    "\n",
    "    finally:\n",
    "        dbConnection.close();\n",
    "        \n",
    "def vectorise_sentence(sentence_list, MODEL = sentence_model):\n",
    "    \n",
    "    INIT_TIME = time.time()\n",
    "    \n",
    "    dict_return = {}\n",
    "    array_vectors = MODEL.encode(sentence_list)\n",
    "    \n",
    "    dict_return['sentence'] = sentence_list\n",
    "    dict_return['vector'] = array_vectors\n",
    "    \n",
    "    FINAL_TIME = time.time() - INIT_TIME\n",
    "    print(f'Vectorisation finished. Total time: {FINAL_TIME:.2f} seconds')\n",
    "    \n",
    "    return dict_return;\n",
    "\n",
    "def calculate_similarity(dict_taxonomy, dict_ingredients):\n",
    "\n",
    "    # Calculate similarity\n",
    "    array_similarity = cosine_similarity(dict_ingredients['vector'], dict_taxonomy['vector'])\n",
    "\n",
    "    # Get best match\n",
    "    best_score_idx = np.argmax(array_similarity, axis = 1)\n",
    "\n",
    "    # Assign ingredient and score to df\n",
    "    matched_ingredient = [dict_taxonomy['sentence'][i] for i in best_score_idx]\n",
    "    matched_score = [array_similarity[i][j] for i, j in enumerate(best_score_idx)]\n",
    "\n",
    "    dict_return = {'ingredient': dict_ingredients['sentence'],\n",
    "                  'matched_ingredient': matched_ingredient,\n",
    "                  'matched_score': matched_score}\n",
    "    \n",
    "    return dict_return;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc0785",
   "metadata": {},
   "source": [
    "## From this point, the experiment begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00065088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table successfuly loaded from raw_taxonomy.csv\n",
      "Table successfuly loaded from raw_ingredients.csv\n"
     ]
    }
   ],
   "source": [
    "### Import files to workspace from directory. If file does not exist, from database\n",
    "\n",
    "STRING_CONNECTION = generate_string_connection()\n",
    "\n",
    "# Import taxonomy\n",
    "if file_exists(FILENAME_RAW_TAXONOMY, CSV_PATH):\n",
    "    df_raw_taxonomy = load_df_from_csv(FILENAME_RAW_TAXONOMY, CSV_PATH)\n",
    "    \n",
    "else:\n",
    "    df_raw_taxonomy = load_df_from_sql('kp_ingredient_taxonomy', STRING_CONNECTION)\n",
    "    \n",
    "    df_raw_taxonomy.to_csv(os.path.join(CSV_PATH, FILENAME_RAW_TAXONOMY), index = False)\n",
    "    \n",
    "# Import raw ingredients\n",
    "if file_exists(FILENAME_RAW_INGREDIENTS, CSV_PATH):\n",
    "    df_raw_ingredients = load_df_from_csv(FILENAME_RAW_INGREDIENTS, CSV_PATH)\n",
    "    \n",
    "else:\n",
    "    df_raw_ingredients = load_df_from_sql('kp_ingredients_base_pantry', STRING_CONNECTION)\n",
    "    \n",
    "    df_raw_ingredients.to_csv(os.path.join(CSV_PATH, FILENAME_RAW_INGREDIENTS), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b00df580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table successfuly loaded from raw_ingredients_sample.csv\n"
     ]
    }
   ],
   "source": [
    "### Load sample dataset. If non existant, generate from raw file\n",
    "\n",
    "if file_exists(FILENAME_RAW_INGREDIENTS_SAMPLE, CSV_PATH):\n",
    "    df_sample_ingredients = load_df_from_csv(FILENAME_RAW_INGREDIENTS_SAMPLE, CSV_PATH)\n",
    "    \n",
    "else:\n",
    "    df_sample_ingredients = df_raw_ingredients.sample(SAMPLE_SIZE, random_state = RANDOM_SEED)\n",
    "    \n",
    "    df_sample_ingredients.to_csv(os.path.join(CSV_PATH, FILENAME_RAW_INGREDIENTS_SAMPLE), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2ce0b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorisation finished. Total time: 1.88 seconds\n",
      "Vectorisation finished. Total time: 17.15 seconds\n",
      "Finalised with category 101. Moving on.\n",
      "Vectorisation finished. Total time: 1.23 seconds\n",
      "Vectorisation finished. Total time: 55.92 seconds\n",
      "Finalised with category 102. Moving on.\n",
      "Vectorisation finished. Total time: 2.51 seconds\n",
      "Vectorisation finished. Total time: 39.71 seconds\n",
      "Finalised with category 103. Moving on.\n",
      "Vectorisation finished. Total time: 0.63 seconds\n",
      "Vectorisation finished. Total time: 0.24 seconds\n",
      "Finalised with category 105. Moving on.\n",
      "Vectorisation finished. Total time: 2.71 seconds\n",
      "Vectorisation finished. Total time: 25.72 seconds\n",
      "Finalised with category 107. Moving on.\n",
      "Vectorisation finished. Total time: 2.20 seconds\n",
      "Vectorisation finished. Total time: 9.55 seconds\n",
      "Finalised with category 108. Moving on.\n",
      "Vectorisation finished. Total time: 1.76 seconds\n",
      "Vectorisation finished. Total time: 5.94 seconds\n",
      "Finalised with category 109. Moving on.\n",
      "Vectorisation finished. Total time: 1.10 seconds\n",
      "Vectorisation finished. Total time: 4.77 seconds\n",
      "Finalised with category 110. Moving on.\n",
      "Vectorisation finished. Total time: 1.43 seconds\n",
      "Vectorisation finished. Total time: 0.75 seconds\n",
      "Finalised with category 113. Moving on.\n",
      "Vectorisation finished. Total time: 0.69 seconds\n",
      "Vectorisation finished. Total time: 7.07 seconds\n",
      "Finalised with category 144. Moving on.\n",
      "Vectorisation finished. Total time: 1.45 seconds\n",
      "Vectorisation finished. Total time: 10.35 seconds\n",
      "Finalised with category 148. Moving on.\n",
      "Finalised all ingredients. Total time: 194.83 seconds\n"
     ]
    }
   ],
   "source": [
    "### Round 1: vectorise per category and compute scores\n",
    "\n",
    "INIT_TIME = time.time()\n",
    "\n",
    "list_categories = df_raw_taxonomy['category_id'].unique()\n",
    "\n",
    "dict_taxonomy_categorised = {}\n",
    "dict_ingredients_categorised = {}\n",
    "dict_matched_ingredients = {}\n",
    "\n",
    "for CAT in list_categories:\n",
    "    \n",
    "    list_taxonomy_categorised = df_raw_taxonomy[df_raw_taxonomy['category_id'] == CAT]['ingredient_name'].tolist()\n",
    "    list_ingredient_categorised = df_sample_ingredients[df_sample_ingredients['category_id'] == CAT]['ingredient_name'].tolist()\n",
    "    \n",
    "    if len(list_taxonomy_categorised) == 0:\n",
    "        print(f'Did not find any ingredient for category {CAT} in taxonomy. Moving on.')\n",
    "        continue\n",
    "        \n",
    "    elif len(list_ingredient_categorised) == 0:\n",
    "        print(f'Did not find any ingredient for category {CAT} in ingredients. Moving on.')\n",
    "        continue\n",
    "        \n",
    "    dict_taxonomy = vectorise_sentence(list_taxonomy_categorised)\n",
    "    dict_ingredients = vectorise_sentence(list_ingredient_categorised)\n",
    "        \n",
    "    dict_taxonomy_categorised[CAT] = dict_taxonomy\n",
    "    dict_ingredients_categorised[CAT] = dict_ingredients\n",
    "    \n",
    "    dict_matched_ingredients[CAT] = calculate_similarity(dict_taxonomy, dict_ingredients)\n",
    "    \n",
    "    print(f'Finalised with category {CAT}. Moving on.')\n",
    "\n",
    "FINAL_TIME = time.time() - INIT_TIME\n",
    "    \n",
    "print(f'Finalised all ingredients. Total time: {FINAL_TIME:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a6611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create dict to save results\n",
    "\n",
    "dict_results = {'round': [],\n",
    "               'description': [],\n",
    "               'execution_time': [],\n",
    "               'average_score': [],\n",
    "               'ratio_over_80': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d36183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute: 194.83 seconds\n",
      "Average matching score: 0.631\n",
      "Ratio over 0.8: 0.120\n"
     ]
    }
   ],
   "source": [
    "### Round 1: compute results\n",
    "\n",
    "dict_categories = {key: [key] * len(content['ingredient']) for key, content in dict_matched_ingredients.items()}\n",
    "\n",
    "df_category_results = pd.DataFrame(columns = ['category_id', 'ingredient', 'matched_ingredient', 'matched_score'])\n",
    "\n",
    "for CAT in dict_categories.keys():\n",
    "    \n",
    "    DF = pd.DataFrame({'category_id': dict_categories[CAT]} | dict_matched_ingredients[CAT])\n",
    "    df_category_results = pd.concat([df_category_results, DF])\n",
    "    \n",
    "df_category_results.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "AVG_SCORE = df_category_results['matched_score'].mean()\n",
    "RATIO_OVER_80 = sum(df_category_results['matched_score'] >= 0.8) / len(df_category_results)\n",
    "    \n",
    "print(f'Time to compute: {FINAL_TIME:.2f} seconds')\n",
    "print(f'Average matching score: {AVG_SCORE:.3f}')\n",
    "print(f'Ratio over 0.8: {RATIO_OVER_80:.3f}')\n",
    "\n",
    "# Append results\n",
    "dict_results['round'].append(1)\n",
    "dict_results['description'].append('Baseline model')\n",
    "dict_results['execution_time'].append(FINAL_TIME)\n",
    "dict_results['average_score'].append(AVG_SCORE)\n",
    "dict_results['ratio_over_80'].append(RATIO_OVER_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eecf5f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorisation finished. Total time: 15.19 seconds\n",
      "Vectorisation finished. Total time: 171.40 seconds\n",
      "Time to compute: 186.65 seconds\n",
      "Average matching score: 0.663\n",
      "Ratio over 0.8: 0.133\n"
     ]
    }
   ],
   "source": [
    "### Round 2: run all possible combinations\n",
    "\n",
    "INIT_TIME = time.time()\n",
    "\n",
    "list_taxonomy = df_raw_taxonomy['ingredient_name'].tolist()\n",
    "list_ingredient = df_sample_ingredients['ingredient_name'].tolist()\n",
    "\n",
    "dict_taxonomy = vectorise_sentence(list_taxonomy)\n",
    "dict_ingredients = vectorise_sentence(list_ingredient)\n",
    "\n",
    "dict_similarity = calculate_similarity(dict_taxonomy, dict_ingredients)\n",
    "\n",
    "df_all_results = pd.DataFrame(dict_similarity)\n",
    "AVG_SCORE = df_all_results['matched_score'].mean()\n",
    "RATIO_OVER_80 = sum(df_all_results['matched_score'] >= 0.8) / len(df_all_results)\n",
    "\n",
    "FINAL_TIME = time.time() - INIT_TIME\n",
    "\n",
    "print(f'Time to compute: {FINAL_TIME:.2f} seconds')\n",
    "print(f'Average matching score: {AVG_SCORE:.3f}')\n",
    "print(f'Ratio over 0.8: {RATIO_OVER_80:.3f}')\n",
    "\n",
    "# Append results\n",
    "dict_results['round'].append(2)\n",
    "dict_results['description'].append('All combinations')\n",
    "dict_results['execution_time'].append(FINAL_TIME)\n",
    "dict_results['average_score'].append(AVG_SCORE)\n",
    "dict_results['ratio_over_80'].append(RATIO_OVER_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f34bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorisation finished. Total time: 14.92 seconds\n",
      "Vectorisation finished. Total time: 153.62 seconds\n",
      "Time to compute: 168.63 seconds\n",
      "Average matching score: 0.673\n",
      "Ratio over 0.8: 0.154\n"
     ]
    }
   ],
   "source": [
    "### Round 3: remove numbers\n",
    "\n",
    "INIT_TIME = time.time()\n",
    "\n",
    "list_taxonomy = df_raw_taxonomy['ingredient_name'].tolist()\n",
    "list_ingredient = df_sample_ingredients['ingredient_name'].tolist()\n",
    "\n",
    "list_removed_numbers_ingredients = [re.sub(r'[0-9]+', '', i) for i in list_ingredient]\n",
    "\n",
    "dict_taxonomy = vectorise_sentence(list_taxonomy)\n",
    "dict_removed_numbers_ingredients = vectorise_sentence(list_removed_numbers_ingredients)\n",
    "\n",
    "dict_similarity = calculate_similarity(dict_taxonomy, dict_removed_numbers_ingredients)\n",
    "\n",
    "df_noNumbers_results = pd.DataFrame(dict_similarity)\n",
    "AVG_SCORE = df_noNumbers_results['matched_score'].mean()\n",
    "RATIO_OVER_80 = sum(df_noNumbers_results['matched_score'] >= 0.8) / len(df_noNumbers_results)\n",
    "\n",
    "FINAL_TIME = time.time() - INIT_TIME\n",
    "\n",
    "print(f'Time to compute: {FINAL_TIME:.2f} seconds')\n",
    "print(f'Average matching score: {AVG_SCORE:.3f}')\n",
    "print(f'Ratio over 0.8: {RATIO_OVER_80:.3f}')\n",
    "\n",
    "# Append results\n",
    "dict_results['round'].append(3)\n",
    "dict_results['description'].append('Numbers removed')\n",
    "dict_results['execution_time'].append(FINAL_TIME)\n",
    "dict_results['average_score'].append(AVG_SCORE)\n",
    "dict_results['ratio_over_80'].append(RATIO_OVER_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05ffc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorisation finished. Total time: 14.88 seconds\n",
      "Vectorisation finished. Total time: 118.73 seconds\n",
      "Time to compute: 133.67 seconds\n",
      "Average matching score: 0.671\n",
      "Ratio over 0.8: 0.151\n"
     ]
    }
   ],
   "source": [
    "### Round 4: remove numbers and signs\n",
    "\n",
    "INIT_TIME = time.time()\n",
    "\n",
    "list_taxonomy = df_raw_taxonomy['ingredient_name'].tolist()\n",
    "list_ingredient = df_sample_ingredients['ingredient_name'].tolist()\n",
    "\n",
    "list_removed_numbers_ingredients = [re.sub(r'[^A-Za-z ]+', '', i) for i in list_ingredient]\n",
    "\n",
    "dict_taxonomy = vectorise_sentence(list_taxonomy)\n",
    "dict_removed_numbers_ingredients = vectorise_sentence(list_removed_numbers_ingredients)\n",
    "\n",
    "dict_similarity = calculate_similarity(dict_taxonomy, dict_removed_numbers_ingredients)\n",
    "\n",
    "df_noNumbersAndSigns_results = pd.DataFrame(dict_similarity)\n",
    "AVG_SCORE = df_noNumbersAndSigns_results['matched_score'].mean()\n",
    "RATIO_OVER_80 = sum(df_noNumbersAndSigns_results['matched_score'] >= 0.8) / len(df_noNumbersAndSigns_results)\n",
    "\n",
    "FINAL_TIME = time.time() - INIT_TIME\n",
    "\n",
    "print(f'Time to compute: {FINAL_TIME:.2f} seconds')\n",
    "print(f'Average matching score: {AVG_SCORE:.3f}')\n",
    "print(f'Ratio over 0.8: {RATIO_OVER_80:.3f}')\n",
    "\n",
    "# Append results\n",
    "dict_results['round'].append(4)\n",
    "dict_results['description'].append('Remove all but characters and whitespace')\n",
    "dict_results['execution_time'].append(FINAL_TIME)\n",
    "dict_results['average_score'].append(AVG_SCORE)\n",
    "dict_results['ratio_over_80'].append(RATIO_OVER_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d9cea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorisation finished. Total time: 24.48 seconds\n"
     ]
    }
   ],
   "source": [
    "### Round 5: remove frequent, uninformative words\n",
    "\n",
    "INIT_TIME = time.time()\n",
    "\n",
    "list_taxonomy = df_raw_taxonomy['ingredient_name'].tolist()\n",
    "list_ingredient = df_sample_ingredients['ingredient_name'].tolist()\n",
    "\n",
    "# Remove numbers\n",
    "list_removed_numbers_ingredients = [re.sub(r'[^A-Za-z ]+', '', i) for i in list_ingredient] # Preserve only letters and whitespace\n",
    "list_removed_numbers_ingredients = [re.sub(' +', ' ', i) for i in list_removed_numbers_ingredients] # Remove double whitespace\n",
    "\n",
    "list_ingredient_words = [i.split(' ') for i in list_removed_numbers_ingredients]\n",
    "list_ingredients_flatten = [food.lower().strip() for sublist in list_ingredient_words for food in sublist]\n",
    "\n",
    "list_taxonomy_words = [i.split('-') for i in list_taxonomy]\n",
    "list_taxonomy_flatten = [food.lower() for sublist in list_taxonomy_words for food in sublist]\n",
    "\n",
    "df_word_count_ingredients = pd.DataFrame(pd.value_counts(np.array(list_ingredients_flatten)))\n",
    "df_word_count_taxonomy = pd.DataFrame(pd.value_counts(np.array(list_taxonomy_flatten)))\n",
    "\n",
    "# Remove used words in taxonomy\n",
    "df_word_count_ingredients.reset_index(inplace = True)\n",
    "df_word_count_ingredients.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "\n",
    "df_word_count_taxonomy.reset_index(inplace = True)\n",
    "df_word_count_taxonomy.rename(columns = {'index': 'word', 0: 'count'}, inplace = True)\n",
    "\n",
    "df_word_count_ingredients = pd.merge(df_word_count_ingredients,\n",
    "                                    df_word_count_taxonomy[['word']],\n",
    "                                    how = 'left',\n",
    "                                    on = 'word',\n",
    "                                    indicator = True)\n",
    "\n",
    "df_word_count_ingredients = df_word_count_ingredients[df_word_count_ingredients['_merge'] == 'left_only']\n",
    "df_word_count_ingredients.set_index('word', inplace = True)\n",
    "\n",
    "# Plot wordcloud\n",
    "# dict_frequencies = df_word_count_ingredients['count'].to_dict()\n",
    "\n",
    "# fig = plt.figure(1, (13, 7))\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# wordcloud = WordCloud(max_font_size = 50,\n",
    "#                       max_words = 100,\n",
    "#                       background_color = 'white').generate_from_frequencies(dict_frequencies)\n",
    "\n",
    "# ax.imshow(wordcloud, interpolation = 'bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# Clean stopwords\n",
    "list_stopwords = df_word_count_ingredients.index.tolist()\n",
    "dict_stopwords = vectorise_sentence(list_stopwords)\n",
    "\n",
    "dict_similarity_stopwords = calculate_similarity(dict_taxonomy, dict_stopwords)\n",
    "df_stopwords = pd.DataFrame(dict_similarity_stopwords)\n",
    "\n",
    "df_stopwords_out = pd.merge(df_word_count_ingredients[['count']],\n",
    "                           df_stopwords,\n",
    "                           how = 'left',\n",
    "                           left_index = True,\n",
    "                           right_on = 'ingredient')\n",
    "\n",
    "if not file_exists(FILENAME_STOPWORDS_EXCEL, EXCEL_PATH):\n",
    "    df_stopwords_out.to_excel(os.path.join(EXCEL_PATH, FILENAME_STOPWORDS_EXCEL), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61d31093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorisation finished. Total time: 12.66 seconds\n",
      "Vectorisation finished. Total time: 82.70 seconds\n",
      "Time to compute: 95.52 seconds\n",
      "Average matching score: 0.719\n",
      "Ratio over 0.8: 0.294\n"
     ]
    }
   ],
   "source": [
    "# Round 5. Execution without stopwords\n",
    "\n",
    "INIT_TIME = time.time()\n",
    "\n",
    "# Import stopwords\n",
    "if file_exists(FILENAME_STOPWORDS_JSON, JSON_PATH):\n",
    "    with open(os.path.join(JSON_PATH, FILENAME_STOPWORDS_JSON), 'r') as f:\n",
    "        dict_stopwords = json.load(f)\n",
    "\n",
    "# Create lists\n",
    "list_taxonomy = df_raw_taxonomy['ingredient_name'].tolist()\n",
    "list_ingredient = df_sample_ingredients['ingredient_name'].tolist()\n",
    "\n",
    "# Process ingredients list\n",
    "list_removed_numbers_ingredients = [re.sub(r'[^A-Za-z ]+', ' ', i) for i in list_ingredient] # Remove all but letters and spaces\n",
    "list_removed_numbers_ingredients = [re.sub(r' +', ' ', i) for i in list_removed_numbers_ingredients] # Remove double spaces\n",
    "list_removed_numbers_ingredients = [i.lower() for i in list_removed_numbers_ingredients] # lower caps\n",
    "\n",
    "# Remove stopwords\n",
    "double_list_ingredients = [i.split(' ') for i in list_removed_numbers_ingredients]\n",
    "double_list_ingredients_sliced = [[i for i in sublist if i not in dict_stopwords['stopwords']] for sublist in double_list_ingredients]\n",
    "\n",
    "# Reconstruct cleaned list\n",
    "list_ingredients_clean = [' '.join(i).strip() for i in double_list_ingredients_sliced]\n",
    "\n",
    "# Encode lists\n",
    "dict_taxonomy = vectorise_sentence(list_taxonomy)\n",
    "dict_ingredients_clean = vectorise_sentence(list_ingredients_clean)\n",
    "\n",
    "dict_similarity = calculate_similarity(dict_taxonomy, dict_ingredients_clean)\n",
    "\n",
    "df_clean_results = pd.DataFrame(dict_similarity)\n",
    "df_clean_results['original_ingredient'] = list_ingredient\n",
    "\n",
    "AVG_SCORE = df_clean_results['matched_score'].mean()\n",
    "RATIO_OVER_80 = sum(df_clean_results['matched_score'] >= 0.8) / len(df_clean_results)\n",
    "\n",
    "FINAL_TIME = time.time() - INIT_TIME\n",
    "\n",
    "print(f'Time to compute: {FINAL_TIME:.2f} seconds')\n",
    "print(f'Average matching score: {AVG_SCORE:.3f}')\n",
    "print(f'Ratio over 0.8: {RATIO_OVER_80:.3f}')\n",
    "\n",
    "# Append results\n",
    "dict_results['round'].append(5)\n",
    "dict_results['description'].append('Remove unwanted words')\n",
    "dict_results['execution_time'].append(FINAL_TIME)\n",
    "dict_results['average_score'].append(AVG_SCORE)\n",
    "dict_results['ratio_over_80'].append(RATIO_OVER_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc301512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>description</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>average_score</th>\n",
       "      <th>ratio_over_80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Baseline model</td>\n",
       "      <td>194.833881</td>\n",
       "      <td>0.630795</td>\n",
       "      <td>0.1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>All combinations</td>\n",
       "      <td>186.653683</td>\n",
       "      <td>0.662786</td>\n",
       "      <td>0.1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Numbers removed</td>\n",
       "      <td>168.631898</td>\n",
       "      <td>0.672659</td>\n",
       "      <td>0.1544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Remove all but characters and whitespace</td>\n",
       "      <td>133.670261</td>\n",
       "      <td>0.670832</td>\n",
       "      <td>0.1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Remove unwanted words</td>\n",
       "      <td>95.517525</td>\n",
       "      <td>0.719051</td>\n",
       "      <td>0.2936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round                               description  execution_time  \\\n",
       "0      1                            Baseline model      194.833881   \n",
       "1      2                          All combinations      186.653683   \n",
       "2      3                           Numbers removed      168.631898   \n",
       "3      4  Remove all but characters and whitespace      133.670261   \n",
       "4      5                     Remove unwanted words       95.517525   \n",
       "\n",
       "   average_score  ratio_over_80  \n",
       "0       0.630795         0.1196  \n",
       "1       0.662786         0.1332  \n",
       "2       0.672659         0.1544  \n",
       "3       0.670832         0.1506  \n",
       "4       0.719051         0.2936  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create dataframe with result metrics\n",
    "\n",
    "df_results = pd.DataFrame(dict_results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600a418",
   "metadata": {},
   "source": [
    "File examples for automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95bbdf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorisation finished. Total time: 13.03 seconds\n"
     ]
    }
   ],
   "source": [
    "### Prepare vectorised ingredients file\n",
    "\n",
    "list_taxonomy = df_raw_taxonomy['ingredient_name'].tolist()\n",
    "\n",
    "dict_taxonomy = vectorise_sentence(list_taxonomy)\n",
    "\n",
    "sr_taxonomy = pd.Series(data = df_raw_taxonomy['id'].tolist(), index = df_raw_taxonomy['ingredient_name'])\n",
    "dict_tax_ids = sr_taxonomy.to_dict()\n",
    "\n",
    "dict_taxonomy_vectorised = {dict_tax_ids[content]: list(dict_taxonomy['vector'][pos].astype(float)) for pos, content in enumerate(dict_taxonomy['sentence'])}\n",
    "\n",
    "with open(f'{JSON_PATH}/taxonomy_vectorised.json', 'w') as F:\n",
    "    json.dump(dict_taxonomy_vectorised, F, indent = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f0286d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Samples of input and output\n",
    "\n",
    "df_io_examples = df_clean_results.sample(10, random_state = 888)\n",
    "df_io_examples = pd.merge(df_io_examples,\n",
    "                         df_raw_taxonomy[['id', 'ingredient_name']],\n",
    "                         how = 'left',\n",
    "                         left_on = 'matched_ingredient',\n",
    "                         right_on = 'ingredient_name')\n",
    "\n",
    "# Input\n",
    "dict_p1_input = {'body': df_io_examples['original_ingredient'].tolist()}\n",
    "\n",
    "with open(f'{JSON_PATH}/P1_input.json', 'w') as F:\n",
    "    json.dump(dict_p1_input, F, indent = 1)\n",
    "    \n",
    "# Output\n",
    "dict_io_examples = df_io_examples[['original_ingredient', 'id', 'matched_score']].to_dict(orient = 'tight')\n",
    "dict_p1_output = {i: dict_io_examples['data'][i] for i in dict_io_examples['index']}\n",
    "\n",
    "with open(f'{JSON_PATH}/P1_output.json', 'w') as F:\n",
    "    json.dump(dict_p1_output, F, indent = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea810179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
